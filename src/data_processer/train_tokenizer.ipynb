{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import BertTokenizerFast, LongformerTokenizerFast, BigBirdTokenizerFast\n",
    "from datasets import DatasetDict, Features, Value, ClassLabel, load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", max_length=4096)\n",
    "\n",
    "tokenizer_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='google/bigbird-roberta-base', vocab_size=50358, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bigbird = BigBirdTokenizerFast.from_pretrained('google/bigbird-roberta-base', max_length=4096)\n",
    "\n",
    "tokenizer_bigbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='allenai/longformer-base-4096', vocab_size=50265, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_longformer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length=4096)\n",
    "\n",
    "tokenizer_longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_name='refined_patents', seed=42, partitions={'train':0.8, 'test_validation':0.2}):\n",
    "    print(\"Loading dataset from csv...\")\n",
    "    features = Features({   'patent_id': Value('string'),\n",
    "                            'text': Value('string'),\n",
    "                            'labels': ClassLabel(names=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]),\n",
    "                            'ipc_class': Value('string'),\n",
    "                            'subclass': Value('string'),\n",
    "                        })\n",
    "                        \n",
    "    data_files = '../../data/'+data_name+'/chunks/*.csv' # add preprocess forlder\n",
    "\n",
    "    dataset = load_dataset('csv', data_files=data_files, features=features, cache_dir='data/'+data_name+'/cache')\n",
    "    dataset = dataset['train'].train_test_split(test_size=partitions['test_validation'], shuffle=True, seed=seed)\n",
    "    test_val = dataset['test'].train_test_split(test_size=0.5, shuffle=True, seed=seed)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': dataset['train'],\n",
    "        'validation': test_val['train'],\n",
    "        'test': test_val['test']\n",
    "    })\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../tokenizers/bert_trained_on_patent_data\\\\tokenizer_config.json',\n",
       " '../../tokenizers/bert_trained_on_patent_data\\\\special_tokens_map.json',\n",
       " '../../tokenizers/bert_trained_on_patent_data\\\\vocab.txt',\n",
       " '../../tokenizers/bert_trained_on_patent_data\\\\added_tokens.json',\n",
       " '../../tokenizers/bert_trained_on_patent_data\\\\tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus = get_training_corpus(dataset['train'])\n",
    "trained_tokenizer_bert = tokenizer_bert.train_new_from_iterator(training_corpus, 50000)\n",
    "trained_tokenizer_bert.save_pretrained(\"../../tokenizers/bert_trained_on_patent_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"../../tokenizers/bert_trained_on_patent_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('long_doc_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68aa273fc7d4ca4d3445a3e4a44b481b7e619f69c0e47033a8637dc88caf9f63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
