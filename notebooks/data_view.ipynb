{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import load_from_disk\n",
    "from transformers import LongformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = load_from_disk(\"../data/refined_patents/tokenized/longformer_tokenizer/train\")\n",
    "\n",
    "df = pd.DataFrame(columns=['patent_id','text', 'labels'])\n",
    "\n",
    "df['patent_id'] = tokenized_data['patent_id'][:10000]\n",
    "df['labels'] = tokenized_data['labels'][:10000]\n",
    "df['text'] = tokenized_data['input_ids'][:10000]\n",
    "del tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = pd.read_pickle('../data/refined_patents/tfidf/longformer_tokenizer/train_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = general[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_based = pd.read_pickle('../data/refined_patents/tfidf/label_based/train_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:43<00:00, 35.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16010481436252594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s=0\n",
    "for i in tqdm(range(general.shape[0])):\n",
    "    id = [general.loc[i].patent_id]\n",
    "    general_tfidf_sum = general.loc[i][1:].sum()\n",
    "    label_based_tfidf_sum = label_based[label_based.patent_id.isin(id)].iloc[0][1:].sum()\n",
    "    s += abs(general_tfidf_sum-label_based_tfidf_sum)\n",
    "print(s/general.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = general.drop(columns=['patent_id']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vecs = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = clusters\n",
    "df['x0'] = x0\n",
    "df['x1'] = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = general.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X).groupby(clusters).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['latable', 'Ġdont', 'Ġvirtues', 'rella', 'Ġdocks', 'Ġmarriage', 'Ġcaves', 'erous', 'Ġseaf', 'Ġeerie', 'Ġpunishments', 'Ġbona', 'Ġstripe', 'Ġnefarious', 'iscover', 'Ġfamous', 'Ġclerks', 'testing', 'Ġcollapses', 'Ġrarity']\n",
      "\n",
      "Cluster 1\n",
      "['latable', 'relations', 'Ġengulf', 'Ġacad', 'Ġship', 'Ġhops', 'thening', 'Ġrecollection', 'rapnel', 'ateral', 'Ġanew', 'Ġfoliage', 'Ġnumer', 'itably', 'der', 'Ġfatig', 'wolf', 'vice', 'Ġhilar', 'Ġeyed']\n",
      "\n",
      "Cluster 2\n",
      "['latable', 'Ġbearer', 'Ġbegins', 'Ġflips', 'Ġcurl', 'rael', 'udden', 'Ġcirculate', 'Ġraven', 'Ġbraces', 'cerned', 'Ġmisunderstand', 'Ġreinforcement', 'Ġmonths', 'Ġuploading', 'allion', 'Ġecstasy', 'fail', 'Ġgorilla', 'orc']\n",
      "\n",
      "Cluster 3\n",
      "['Ġfleet', 'Ġeste', 'orie', 'Ġbalcony', 'Ġincredibly', 'Ġinsightful', 'Ġsolemn', 'ahon', 'Ġsmuggled', 'Ġhated', 'Ġcondemnation', 'Ġeater', 'Ġstaffer', 'intelligence', 'Ġsubmitting', 'uten', 'Ġsubsistence', 'arrell', 'Ġhardships', 'Ġinning']\n",
      "\n",
      "Cluster 4\n",
      "['latable', 'Ġhilar', 'Ġeyed', 'ool', 'indu', 'Ġgenders', 'washing', 'rowth', 'Ġoptimizing', 'Ġfrail', 'dress', 'resp', 'Ġproficient', 'iggins', 'Ġcaul', 'Ġhamm', 'Ġirresistible', 'Ġdecad', 'mental', 'Ġcuff']\n",
      "\n",
      "Cluster 5\n",
      "['inus', 'Ġonwards', 'Ġfrank', 'olutely', 'Ġcens', 'Ġmiss', 'Ġunob', 'ruit', 'ompl', 'Ġshareholders', 'Ġstagnant', 'astical', 'silver', 'Ġlobb', 'Ġelf', 'Ġclan', 'Ġprops', 'qualified', 'Ġmistakenly', 'Ġaddicts']\n",
      "\n",
      "Cluster 6\n",
      "['Ġspawned', 'Ġscept', 'astical', 'ripp', 'Ġlobb', 'Ġskinny', 'Ġmaj', 'innon', 'Ġvolley', 'Ġancestor', 'Ġgarg', 'Ġcabal', 'Ġideals', 'Ġbegging', 'Ġdomest', 'iola', 'asley', 'Ġshareholders', 'berries', 'ourage']\n",
      "\n",
      "Cluster 7\n",
      "['Ġobsolete', 'Ġadjud', 'Ġroommate', 'capital', 'Ġdues', 'oried', 'Ġtho', 'Ġpremie', 'Ġoffensive', 'Ġperennial', 'Ġironic', 'ichael', 'Ġwors', 'gary', 'eking', 'Ġaltru', 'Ġsafeguards', 'Ġdwindling', 'torn', 'ridge']\n"
     ]
    }
   ],
   "source": [
    "#Preprocessed\n",
    "for i,r in data.iterrows():\n",
    "    print('\\nCluster {}'.format(i))\n",
    "    ids = [int(terms[t]) for t in np.argsort(r)[:20]]\n",
    "\n",
    "    print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['Ġnav', 'Ġsparkling', 'Ġ1945', 'allas', 'existence', 'Ġfasc', 'Ġexpectations', 'stration', 'Ġdissemination', 'Ġlantern', 'alis', 'irled', 'utra', 'dule', '467', 'enforcement', 'Ġtweet', 'Ġlegends', 'Ġdab', 'Ġpric']\n",
      "\n",
      "Cluster 1\n",
      "['Ġnav', 'Ġdirectives', 'olerance', 'Ġassailants', 'ussen', 'Ġaggrav', 'Ġtame', 'Ġunfit', 'Ġpastry', 'cards', 'Ġdefy', 'Ġquickest', 'Ġupheaval', 'Ġspoiled', 'baby', 'athom', 'umbs', 'affles', 'ryan', 'Ġepidem']\n",
      "\n",
      "Cluster 2\n",
      "['eland', 'neau', 'Ġsir', 'Ġjealousy', 'ardon', 'Ġpsych', 'Ġunin', 'quist', 'eful', 'iscovery', 'idy', 'yx', 'Ġinvaluable', 'Ġtranscription', 'Ġproxies', 'Ġservings', 'alian', 'Ġhonor', 'Ġthreatened', 'Ġpra']\n",
      "\n",
      "Cluster 3\n",
      "['eland', 'Ġdistressed', 'istent', 'container', 'Ġpayable', 'ounters', 'Ġclimate', 'bably', 'izabeth', 'Ġquint', 'Ġstray', 'inctions', 'Ġelusive', 'Ġgrandma', 'Ġgadgets', 'Ġlandslide', 'assert', 'Ġfaintly', 'license', 'gets']\n",
      "\n",
      "Cluster 4\n",
      "['Ġnav', '393', 'irled', 'pun', 'Ġlantern', 'É', 'Ġpigs', 'approved', 'Ġdissemination', 'stration', 'Ġidentifiable', 'Ġtweet', 'Ġmashed', 'Ġpric', 'Ġdra', 'Ġearthquakes', 'speaking', 'lear', 'Ġrefin', 'Ġunto']\n",
      "\n",
      "Cluster 5\n",
      "['Ġnav', 'Ġseminars', 'Ġchats', 'Ġcampus', 'Ġdeed', 'Ġsnowy', 'Ġcountdown', 'Ġtopple', 'aylor', 'anni', 'igrated', 'ennis', 'Ġallocations', 'Ġbiodiversity', 'Ġtrait', 'Ġfinance', 'Ġracer', 'flying', 'Ġjail', 'patrick']\n",
      "\n",
      "Cluster 6\n",
      "['Ġnav', 'Ġfingertips', 'vu', 'Ġans', 'plug', 'Ġforc', 'Ġguarant', 'addle', 'unders', 'Ġtides', 'Ġcountering', 'raper', 'ursions', 'raised', 'Ġreload', 'ourses', 'Ġwand', 'Ġledger', 'Ġcompile', 'Ġ1932']\n",
      "\n",
      "Cluster 7\n",
      "['Ġnav', 'Ġacne', 'akable', 'Ġbred', 'Ġexcerpt', 'Ġcher', 'Ġambush', 'Ġgiveaway', 'izon', 'Ġdisagrees', 'llan', 'Ġhandwritten', 'Ã¦', 'Ġmonkeys', 'Ġnutritious', 'piring', 'Ġdrown', 'graduate', 'Ġprosper', 'olutions']\n"
     ]
    }
   ],
   "source": [
    "#Classic\n",
    "for i,r in data.iterrows():\n",
    "    print('\\nCluster {}'.format(i))\n",
    "    ids = [int(terms[t]) for t in np.argsort(r)[:20]]\n",
    "\n",
    "    print(tokenizer.convert_ids_to_tokens(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73216522d7dc07982c48c608876be0f984b87cbd9b593e185fee1fc5b47e2777"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('long_doc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
